# platform_service/core.py

# Standard library imports
import os
import json
import logging
import csv
import time as _time
import threading
from typing import TypedDict, List, Dict, Any, Optional
from pathlib import Path

# Third-party imports
from dotenv import load_dotenv
from konlpy.tag import Okt

# LangChain & LangGraph ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain import hub
from langchain.agents import AgentExecutor, create_react_agent
from langchain.prompts import PromptTemplate
from langchain.schema import Document
from langchain.tools import tool
from langchain_community.document_loaders import (
    PyPDFLoader, CSVLoader, TextLoader, Docx2txtLoader
)
from langchain_community.vectorstores import FAISS
from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import StateGraph, END

# LangSmithë¥¼ ìœ„í•œ CallbackManager ì„í¬íŠ¸ (python ë²„ì „ ë‚®ì¶°ì•¼í•´ì„œ hold - 3.11)
# from langchain.callbacks.manager import CallbackManager
# from langchain_community.callbacks.langsmith import LangSmithCallbackHandler

# Local application imports
from . import constants

# ì¤‘ì•™ ì„¤ì •(logging_config.py)ì—ì„œ ì„¸íŒ…ëœ ë¡œê±° ë¶ˆëŸ¬ì˜¤ê¸°
logger = logging.getLogger(__name__)

# =============================================================
# 1. ê³µí†µ ì„¤ì • / í™˜ê²½ ë³€ìˆ˜
# =============================================================
load_dotenv()

# Azure OpenAI í™˜ê²½ë³€ìˆ˜
AOAI_ENDPOINT = os.getenv("AOAI_ENDPOINT", "")
AOAI_API_KEY = os.getenv("AOAI_API_KEY", "")
AOAI_API_VERSION = os.getenv("AOAI_API_VERSION", "2024-10-21")
AOAI_DEPLOY_GPT4O_MINI = os.getenv("AOAI_DEPLOY_GPT4O_MINI", "gpt-4o-mini")
AOAI_DEPLOY_GPT4O = os.getenv("AOAI_DEPLOY_GPT4O", "gpt-4o")
AOAI_DEPLOY_EMBED_3_SMALL = os.getenv("AOAI_DEPLOY_EMBED_3_SMALL", "text-embedding-3-small")

# Azure ì„¤ì • í™•ì¸ í”Œë˜ê·¸
AZURE_AVAILABLE = bool(AOAI_ENDPOINT and AOAI_API_KEY)
if not AZURE_AVAILABLE:
    logger.warning("Azure OpenAI ì„¤ì •ì´ ì—†ì–´ í´ë°±(Fallback) ëª¨ë“œë¡œ ë™ì‘í•©ë‹ˆë‹¤.")


# =============================================================
# 2. ê³µí†µ ìœ í‹¸
# workflow/utils.py (í˜•íƒœì†Œ ë¶„ì„ê¸°, ê³µìš© ìœ í‹¸)
# =============================================================
# Okt í˜•íƒœì†Œ ë¶„ì„ê¸° - Lazy Initialization (Thread-Safe)
_okt = None
_okt_lock = threading.Lock()

_faq_data = None   # FAQ ë°ì´í„° ìºì‹œ ì „ì—­ë³€ìˆ˜
_owner_data = None


class _DummyOkt:
    """pytest ì „ìš©: JVM ì—†ì´ ìµœì†Œ ê¸°ëŠ¥ë§Œ ì œê³µí•˜ëŠ” ë”ë¯¸ ë¶„ì„ê¸°"""
    def phrases(self, text: str):
        return [w for w in text.lower().split() if w]
    def nouns(self, text: str):
        return [w for w in text.lower().split() if w]


def get_okt():
    """
    Okt ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    JVMì€ í”„ë¡œì„¸ìŠ¤ì—ì„œ í•œ ë²ˆë§Œ ì‹¤í–‰ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ
    thread-safe lazy init íŒ¨í„´ì„ ì ìš©í–ˆìŠµë‹ˆë‹¤.
    í…ŒìŠ¤íŠ¸ ì‹œì—ëŠ” í™˜ê²½ë³€ìˆ˜ TEST_DISABLE_JVM=1ì´ë©´ ë”ë¯¸ OKTë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    global _okt
    if _okt is None:
        with _okt_lock:  # ë‹¤ë¥¸ ìŠ¤ë ˆë“œì™€ ë™ì‹œ ì‹¤í–‰ ë°©ì§€
            if _okt is None:  # double-checked locking
                if os.getenv("TEST_DISABLE_JVM", "0") == "1":
                    _okt = _DummyOkt()       # â† JVM ë¯¸ê¸°ë™
                else:
                    _okt = Okt()             # â† ì‹¤ì œ JVM ê¸°ë™
    return _okt


# # Okt í˜•íƒœì†Œ ë¶„ì„ ì‹±ê¸€í†¤ íŒ¨í„´ ì ìš©
# class SingletonOkt:
#     _instance = None
#     _lock = threading.Lock() # ìŠ¤ë ˆë“œ ì ê¸ˆ ê°ì²´
# 
#     def __new__(cls):
#         with cls._lock: # ë½(lock)ì„ ì‚¬ìš©í•´ ìŠ¤ë ˆë“œë¡œë¶€í„° ì•ˆì „í•˜ê²Œ ì ‘ê·¼
#             if cls._instance is None:
#                 try:
#                     # Okt ê°ì²´ëŠ” í•œ ë²ˆë§Œ ìƒì„±
#                     cls._instance = Okt()
#                     logger.info("Okt ê°ì²´ê°€ ì„±ê³µì ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
#                 except Exception as e:
#                     logger.error(f"Okt ê°ì²´ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
#                     raise RuntimeError("Okt ê°ì²´ ì´ˆê¸°í™” ì‹¤íŒ¨") from e
#         return cls._instance
# 
# # Okt ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ ì‹œì ì— í•œ ë²ˆë§Œ ë¯¸ë¦¬ ìƒì„±
# _okt_instance = None
# def get_okt():
#     """Okt ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë°˜í™˜ (Okt ê°ì²´ëŠ” í•œ ë²ˆë§Œ ìƒì„±)"""
#     global _okt_instance
#     if _okt_instance is None:
#         _okt_instance = SingletonOkt() # ìˆ˜ì •ëœ SingletonOkt í´ë˜ìŠ¤ ì‚¬ìš©
#     return _okt_instance

# =============================================================
# 3. RAG ë° LLM ê´€ë ¨ í•¨ìˆ˜ ì •ì˜
# workflow/vectorstore.py (ë²¡í„°ìŠ¤í† ì–´ ê´€ë¦¬)
# =============================================================
# ì„ë² ë”© ëª¨ë¸ ìƒì„±
def _make_embedder() -> AzureOpenAIEmbeddings:
    if not AZURE_AVAILABLE:
        raise RuntimeError("Azure OpenAI ì„¤ì •ì´ ì—†ì–´ Embedderë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    return AzureOpenAIEmbeddings(
        azure_deployment=AOAI_DEPLOY_EMBED_3_SMALL,
        api_key=AOAI_API_KEY,
        azure_endpoint=AOAI_ENDPOINT,
        api_version=AOAI_API_VERSION,
    )

# RAG - ì›ë³¸ ë°ì´í„° ìˆ˜ì§‘ ë° ì „ì²˜ë¦¬ ë¡œì§ [checklist: 6]
def _load_docs_from_kb() -> List[Document]:
    """
    RAG - ì›ë³¸ ë°ì´í„° ìˆ˜ì§‘ ë° ì „ì²˜ë¦¬ ë¡œì§
    """
    docs: List[Document] = []
    # FAQ ë¬¸ì„œ ì¶”ê°€
    faq_data = load_faq_data()
    if faq_data:
        docs.extend([
            Document(
                page_content=f"ì§ˆë¬¸: {item.get('question')}\në‹µë³€: {item.get('answer')}",
                metadata={"source": "faq_data.csv"}
            ) for item in faq_data
        ])
    # ì—…ë¬´ ë‹´ë‹¹ì ë¬¸ì„œ ì¶”ê°€
    owner_data = load_owner_data()
    if owner_data:
        docs.extend([
            Document(
                page_content=f"í™”ë©´: {item.get('screen')}\në‹´ë‹¹ì: {item.get('owner')}\nì´ë©”ì¼: {item.get('email')}\nì—°ë½ì²˜: {item.get('phone')}",
                metadata={"source": "owners.csv"}
            ) for item in owner_data
        ])
    # ê¸°íƒ€ ë¬¸ì„œ ë¡œë“œ
    for kb_path in [constants.KB_DEFAULT_DIR, constants.KB_DATA_DIR]:
        if not kb_path.exists():
            kb_path.mkdir(parents=True, exist_ok=True)
            for p in kb_path.rglob("*"):
                # âœ… DB íŒŒì¼(history.db ë“±)ì€ ë²¡í„°í™” ëŒ€ìƒì—ì„œ ì œì™¸
                if not p.is_file():
                    continue
                if p.suffix.lower() == ".db" or p.name == "history.db":
                    continue
                if p.name == "faq_data.csv":
                    continue
                try:
                    suf = p.suffix.lower()
                    if suf == ".pdf": docs.extend(PyPDFLoader(str(p)).load())
                    elif suf == ".csv": docs.extend(CSVLoader(file_path=str(p), encoding="utf-8").load())
                    elif suf in [".txt", ".md"]: docs.extend(TextLoader(str(p), encoding="utf-8").load())
                    elif suf == ".docx": docs.extend(Docx2txtLoader(str(p)).load())
                except Exception as e:
                    logger.warning(f"ë¬¸ì„œ ë¡œë“œ ì‹¤íŒ¨: {p} - {e}")
    return docs

# RAG - FAISS ê¸°ë°˜ì˜ Vector ìŠ¤í† ì–´ êµ¬ì¶• [checklist: 7]
def build_or_load_vectorstore() -> FAISS:
    """
    RAG - FAISS ê¸°ë°˜ì˜ Vector ìŠ¤í† ì–´ êµ¬ì¶•
    """
    if not AZURE_AVAILABLE:
        raise RuntimeError("'Rebuild Index'ëŠ” Azure OpenAI ì„¤ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.")
    embed = _make_embedder()
    if (constants.INDEX_DIR / f"{constants.INDEX_NAME}.faiss").exists():
        return FAISS.load_local(
            str(constants.INDEX_DIR / constants.INDEX_NAME),
            embeddings=embed,
            allow_dangerous_deserialization=True
        )
    raw_docs = _load_docs_from_kb()
    if not raw_docs:
        faq_data = load_faq_data()
        if faq_data:
            raw_docs = [
                Document(
                    page_content=f"ì§ˆë¬¸: {item.get('question')}\në‹µë³€: {item.get('answer')}",
                    metadata={"source": "faq_data.csv"}
                ) for item in faq_data
            ]
            logger.info("ì—…ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ì–´ faq_data.csvë¥¼ ê¸°ë³¸ RAG ì§€ì‹ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        else:
            seed_text = """ì‚¬ë‚´ í—¬í”„ë°ìŠ¤í¬ ì•ˆë‚´
- ID ë°œê¸‰: ì‹ ê·œ ì…ì‚¬ìëŠ” HR í¬í„¸ì—ì„œ 'ê³„ì • ì‹ ì²­' ì–‘ì‹ì„ ì œì¶œ. ìŠ¹ì¸ í›„ ITê°€ ê³„ì • ìƒì„±.
- ë¹„ë°€ë²ˆí˜¸ ì´ˆê¸°í™”: SSO í¬í„¸ì˜ 'ë¹„ë°€ë²ˆí˜¸ ì¬ì„¤ì •' ê¸°ëŠ¥ ì‚¬ìš©. ë³¸ì¸ì¸ì¦ í•„ìš”.
- ë‹´ë‹¹ì ì¡°íšŒ: í¬í„¸ ìƒë‹¨ ê²€ìƒ‰ì°½ì— í™”ë©´/ë©”ë‰´ëª…ì„ ì…ë ¥í•˜ë©´ ë‹´ë‹¹ì ì¹´ë“œê°€ í‘œì‹œë¨."""
            raw_docs = [Document(page_content=seed_text, metadata={"source": "seed-faq.txt"})]

    splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=120)
    chunks = splitter.split_documents(raw_docs)
    constants.INDEX_DIR.mkdir(parents=True, exist_ok=True)
    # FAISSì— ë¬¸ì„œë¥¼ ì„ë² ë”©í•˜ê³  ì €ì¥
    vs = FAISS.from_documents(chunks, embed)
    vs.save_local(str(constants.INDEX_DIR / constants.INDEX_NAME))
    return vs

# RAG - FAISS ë²¡í„° ìŠ¤í† ì–´ ê²€ìƒ‰ê¸° (Singleton Pattern)
_vectorstore: Optional[FAISS] = None
_vectorstore_lock = threading.Lock()

def retriever(k: int = 4):
    """RAG - ë²¡í„°ìŠ¤í† ì–´ retriever (Singleton Pattern)"""
    global _vectorstore
    if _vectorstore is None:
        with _vectorstore_lock:
            if _vectorstore is None:
                _vectorstore = build_or_load_vectorstore()
    return _vectorstore.as_retriever(search_kwargs={"k": k})

# LLM(ì–¸ì–´ ëª¨ë¸) ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±
def make_llm(model: str = AOAI_DEPLOY_GPT4O_MINI, temperature: float = 0.2) -> AzureChatOpenAI:
    """
    Azure OpenAI ì„œë¹„ìŠ¤ì— ì—°ê²°í•˜ì—¬ LLM(ì–¸ì–´ ëª¨ë¸) ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    Args:
        model (str): ì‚¬ìš©í•  Azure OpenAI ë°°í¬ ëª¨ë¸ì˜ ì´ë¦„. ê¸°ë³¸ê°’ì€ gpt-4o-miniì…ë‹ˆë‹¤.
        temperature (float): ëª¨ë¸ì˜ ì°½ì˜ì„±(ë¬´ì‘ìœ„ì„±)ì„ ì¡°ì ˆí•˜ëŠ” ë§¤ê°œë³€ìˆ˜. 0.0ì—ì„œ 2.0 ì‚¬ì´ì˜ ê°’. 
                           ê°’ì´ ë‚®ì„ìˆ˜ë¡ ì˜ˆì¸¡ ê°€ëŠ¥í•˜ê³  ì¼ê´€ëœ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    Returns:
        AzureChatOpenAI: ì„¤ì •ëœ ì–¸ì–´ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤.
    Raises:
        RuntimeError: Azure OpenAI í™˜ê²½ ë³€ìˆ˜(ì—”ë“œí¬ì¸íŠ¸, API í‚¤)ê°€ ì„¤ì •ë˜ì§€ ì•Šì€ ê²½ìš° ë°œìƒ.
    """
    if not AZURE_AVAILABLE:
        raise RuntimeError("Azure OpenAI ì„¤ì •ì´ ì—†ì–´ LLMì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    return AzureChatOpenAI(
        azure_deployment=model,
        api_version=AOAI_API_VERSION,
        azure_endpoint=AOAI_ENDPOINT,
        api_key=AOAI_API_KEY,
        temperature=temperature,
    )


# =============================================================
# 4. LangGraph ë„êµ¬ ì •ì˜
# workflow/tools.py (ë„êµ¬ ì •ì˜)
# =============================================================
class BotState(TypedDict):
    question: str
    intent: str
    reply: str
    sources: List[Dict[str, Any]]
    tool_output: Dict[str, Any]


@tool
def tool_reset_password(payload: Dict[str, Any] = {}) -> Dict[str, Any]:
    """ë¹„ë°€ë²ˆí˜¸ ì´ˆê¸°í™” ì ˆì°¨ë¥¼ ì•ˆë‚´í•©ë‹ˆë‹¤."""
    return {
        "ok": True,
        "message": "ë¹„ë°€ë²ˆí˜¸ ì´ˆê¸°í™” ì ˆì°¨ ì•ˆë‚´",
        "steps": ["SSO í¬í„¸ ì ‘ì† > ë¹„ë°€ë²ˆí˜¸ ì¬ì„¤ì •", "ë³¸ì¸ì¸ì¦", "ìƒˆ ë¹„ë°€ë²ˆí˜¸ ì„¤ì •"]
    }


@tool
def tool_request_id(payload: Dict[str, Any] = {}) -> Dict[str, Any]:
    """ID ë°œê¸‰ ì‹ ì²­ ì ˆì°¨ë¥¼ ì•ˆë‚´í•©ë‹ˆë‹¤."""
    return {
        "ok": True,
        "message": "ID ë°œê¸‰ ì‹ ì²­ ì ˆì°¨ ì•ˆë‚´",
        "steps": ["HR í¬í„¸ ì ‘ì† > 'ê³„ì • ì‹ ì²­' ì–‘ì‹ ì œì¶œ", "ì–‘ì‹ ìŠ¹ì¸ í›„ ITíŒ€ì—ì„œ ê³„ì • ìƒì„±"]
    }


@tool
def tool_owner_lookup(payload: Dict[str, Any]) -> Dict[str, Any]:
    """í™”ë©´ì´ë‚˜ ë©”ë‰´ì˜ ë‹´ë‹¹ì ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤. `screen` ì¸ìê°€ í•„ìš”í•©ë‹ˆë‹¤."""
    try:
        screen = payload.get("screen") or ""
        owner_data = load_owner_data()
        for item in owner_data:
            if screen in item.get("screen", ""):
                return {
                    "ok": True,
                    "screen": item.get("screen"),
                    "owner": {
                        "owner": item.get("owner"),
                        "email": item.get("email"),
                        "phone": item.get("phone")
                    }
                }
        return {"ok": False, "message": f"'{screen}' ë‹´ë‹¹ì ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."}
    except Exception as e:
        return {"ok": False, "message": f"ë„êµ¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"}
# =============================================================
# 5. LangGraph ë…¸ë“œ ì •ì˜
# workflow/nodes.py (LangGraph ë…¸ë“œ ì •ì˜)
# =============================================================
    # Prompt Engineering - í”„ë¡¬í”„íŠ¸ ìµœì í™” (Few-shot Prompting) [checklist: 1]
# ë…¸ë“œ(Node) í•¨ìˆ˜
def node_classify(state: BotState) -> BotState:
    llm = make_llm(model=AOAI_DEPLOY_GPT4O_MINI, temperature=0.1)
    prompt_template = PromptTemplate.from_template("""
    ë‹¹ì‹ ì€ ì‚¬ìš©ì ì˜ë„ë¥¼ ë¶„ë¥˜í•˜ëŠ” AIì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ê°€ì¥ ì í•©í•œ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”.
    - `greeting`: ì‚¬ìš©ìê°€ ì¸ì‚¬ë§("ì•ˆë…•", "ì•ˆë…•í•˜ì„¸ìš”" ë“±)ì„ ê±´ë„¬ ë•Œ
    - `direct_tool`: ì‚¬ìš©ìê°€ íŠ¹ì • ì‹œìŠ¤í…œ ì‘ì—…(ë¹„ë°€ë²ˆí˜¸ ì´ˆê¸°í™”, ID ë°œê¸‰, ë‹´ë‹¹ì ì¡°íšŒ)ì„ ìš”ì²­í•  ë•Œ
    - `faq`: ìì£¼ ë¬»ëŠ” ì§ˆë¬¸(FAQ)ì— ê´€ë ¨ëœ ì§ˆë¬¸ì¼ ë•Œ
    - `general_qa`: ìœ„ì— í•´ë‹¹í•˜ì§€ ì•ŠëŠ” ì¼ë°˜ì ì¸ ì§ˆë¬¸ì¼ ë•Œ

    ì§ˆë¬¸ì— ëŒ€í•œ ë¶„ë¥˜ì™€ í•„ìš”í•œ ì¸ì(JSON í˜•ì‹)ë¥¼ ë°˜í™˜í•˜ì„¸ìš”.
    JSON í˜•ì‹: {{"intent": "ë¶„ë¥˜", "arguments": {{"key": "value"}}}}
    ì˜ˆì‹œ:
    ì‚¬ìš©ì ì§ˆë¬¸: "ë¹„ë°€ë²ˆí˜¸ ì´ˆê¸°í™” ì•Œë ¤ì¤˜" -> {{"intent": "direct_tool", "arguments": {{"tool_name": "tool_reset_password"}}}}
    ì‚¬ìš©ì ì§ˆë¬¸: "ì¸ì‚¬ì‹œìŠ¤í…œ ë‹´ë‹¹ì ëˆ„êµ¬ì•¼?" -> {{"intent": "direct_tool", "arguments": {{"tool_name": "tool_owner_lookup", "screen": "ì¸ì‚¬ì‹œìŠ¤í…œ-ì‚¬ìš©ìê´€ë¦¬"}}}}
    ì‚¬ìš©ì ì§ˆë¬¸: "ì ì‹¬ì‹œê°„ì´ ì–¸ì œì•¼?" -> {{"intent": "faq", "arguments": {{}}}}
    ì‚¬ìš©ì ì§ˆë¬¸: "íšŒì‚¬ ë³µì§€ì œë„ ì„¤ëª…í•´ì¤˜" -> {{"intent": "general_qa", "arguments": {{}}}}
    ì‚¬ìš©ì ì§ˆë¬¸: "ì•ˆë…•" -> {{"intent": "greeting", "arguments": {{}}}}

    ì‚¬ìš©ì ì§ˆë¬¸: "{question}" -> 
    """)
    prompt = prompt_template.format(question=state["question"])
    out = llm.invoke(prompt).content
    
    intent, args = "general_qa", {}
    try:
        data = json.loads(out.strip())
        intent = data.get("intent", "general_qa")
        args = data.get("arguments", {}) or {}
    except json.JSONDecodeError:
        logger.warning(f"[Classifier ì˜¤ë¥˜] JSONDecodeError: {out}")
    except Exception as e:
        logger.error(f"[Classifier ì˜¤ë¥˜] ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜: {out} - {e}")
    
    return {**state, "intent": intent, "tool_output": args}

def node_greeting(state: BotState) -> BotState:
    return {**state, "reply": "ë„¤, ë°˜ê°‘ìŠµë‹ˆë‹¤. ë¬¸ì˜ì‚¬í•­ì„ ë§ì”€í•´ ì£¼ì‹œë©´ ì œê°€ ë„ì™€ë“œë¦´ê²Œìš”.", "sources": []}

def node_direct_tool(state: BotState) -> BotState:
    tool_name = state.get("tool_output", {}).get("tool_name")
    payload = state.get("tool_output", {}).get("arguments", {})
    
    if tool_name == "tool_reset_password":
        res = tool_reset_password.invoke({})
    elif tool_name == "tool_request_id":
        res = tool_request_id.invoke({})
    elif tool_name == "tool_owner_lookup":
        # âœ… ì „ì²´ ì¡°íšŒ ì˜ˆì™¸ ì²˜ë¦¬
        query_text = state.get("question", "")
        if "ì „ì²´" in query_text or "ëª¨ë‘" in query_text or "ë¦¬ìŠ¤íŠ¸" in query_text:
            from platform_service.core import load_owner_data
            owners = load_owner_data()
            res = {
                "ok": True,
                "tool_name": "tool_owner_lookup",
                "owners": owners
            }
        else:
            res = tool_owner_lookup.invoke({"payload": payload})
    else:
        res = {"ok": False, "message": "ì•Œ ìˆ˜ ì—†ëŠ” ë„êµ¬ í˜¸ì¶œ"}
    
    # ë„êµ¬ ê²°ê³¼ì— ì›ë˜ í˜¸ì¶œëœ ë„êµ¬ ì´ë¦„ ì¶”ê°€
    res["tool_name"] = tool_name
    return {**state, "tool_output": res}
    
# ë…¸ë“œ(Node) í•¨ìˆ˜ ìˆ˜ì •
def node_faq(state: BotState) -> BotState:
    # FAQ ë°ì´í„°ëŠ” ì´ë¯¸ RAG íŒŒì´í”„ë¼ì¸ì— í¬í•¨ë˜ì–´ ìˆìœ¼ë¯€ë¡œ,
    # ì´ ë…¸ë“œì—ì„œëŠ” FAQì™€ ê´€ë ¨ëœ ì§ˆë¬¸ì¸ì§€ í™•ì¸ë§Œ í•˜ê³  ë°”ë¡œ RAGë¡œ ë¼ìš°íŒ…
    # (ë‹¨, ì•„ë˜ ë¡œì§ì€ í•„ìš”ì— ë”°ë¼ ì‚­ì œ ë˜ëŠ” ìˆ˜ì •ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
    #  í˜„ì¬ëŠ” RAG ë…¸ë“œì—ì„œ FAQë¥¼ í¬í•¨í•œ ëª¨ë“  ë¬¸ì„œë¥¼ ì²˜ë¦¬í•˜ë„ë¡ ë˜ì–´ ìˆìŠµë‹ˆë‹¤.)
    
    # ì´ ë¶€ë¶„ì„ ì‚­ì œí•˜ê±°ë‚˜, ë” ì´ìƒ FAQë¥¼ ë³„ë„ë¡œ ì²˜ë¦¬í•˜ì§€ ì•Šë„ë¡ ìˆ˜ì •í•©ë‹ˆë‹¤.
    # ê¸°ì¡´ ë¡œì§:
    # faq_answer = find_similar_faq(state["question"])
    # if faq_answer:
    #     return {**state, "tool_output": {"ok": True, "answer": faq_answer.get("answer")}, "intent": "faq", "sources": [{"source": "faq_data.csv"}]}
    # else:
    #     return {**state, "intent": "general_qa"}

    # ê°œì„ ëœ ë¡œì§:
    # FAQ ì§ˆë¬¸ì€ ì¼ë°˜ ì§ˆë¬¸ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ RAG ë…¸ë“œë¡œ ë¼ìš°íŒ…
    return {**state, "intent": "general_qa"}
    # RAG - ì‚¬ì „ ì •ì˜ëœ ë°ì´í„°(ë¬¸ì„œ)ë¥¼ ê²€ìƒ‰í•˜ì—¬ AIì˜ ë…¼ë¦¬ë ¥ì„ ë³´ê°•/ RAG ê¸°ë°˜ ì§€ì‹ ê²€ìƒ‰ ê¸°ëŠ¥ êµ¬í˜„ [checklist: 8,9] 
# Prompt Engineering - í”„ë¡¬í”„íŠ¸ ìµœì í™” (ì—­í•  ë¶€ì—¬ + Chain-of-Thought) [checklist: 1] 
def node_rag(state: BotState) -> BotState:
    docs = retriever(k=4).get_relevant_documents(state["question"])
    
    # ê²€ìƒ‰ ê²°ê³¼ê°€ ì „í˜€ ì—†ìœ¼ë©´ â†’ ì¼ë°˜ LLM ë‹µë³€
    if not docs:
        llm = make_llm(model=AOAI_DEPLOY_GPT4O)
        sys_prompt = "ë„ˆëŠ” ì‚¬ë‚´ í—¬í”„ë°ìŠ¤í¬ ìƒë‹´ì›ì´ë‹¤. ë‚´ë¶€ ë¬¸ì„œì— ì—†ëŠ” ì§ˆë¬¸ì´ë¼ë„ ì¼ë°˜ ì§€ì‹ì„ í™œìš©í•´ í•œêµ­ì–´ë¡œ ë‹µí•´ë¼."
        out = llm.invoke([
            {"role": "system", "content": sys_prompt},
            {"role": "user", "content": state["question"]}
        ]).content
        return {**state, "reply": out, "sources": []}

    # ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆìœ¼ë©´ â†’ RAG ê¸°ë°˜ ë‹µë³€
    context = "\n\n".join([f"[{i+1}] {d.page_content[:1200]}" for i, d in enumerate(docs)])
    sources = [{"index": i+1, "source": d.metadata.get("source","unknown"), "page": d.metadata.get("page")} for i,d in enumerate(docs)]
    llm = make_llm(model=AOAI_DEPLOY_GPT4O)
    sys_prompt = (
        "ë„ˆëŠ” ì‚¬ë‚´ í—¬í”„ë°ìŠ¤í¬ ìƒë‹´ì›ì´ë‹¤. "
        "ê°€ëŠ¥í•˜ë©´ ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ í™œìš©í•´ ë‹µë³€í•˜ë˜, "
        "ë¶€ì¡±í•˜ë©´ ì¼ë°˜ ì§€ì‹ì„ ë³´ì™„í•´ì„œ ë‹µí•´ë¼. "
        "ì¶”ê°€ í™•ì¸ì´ í•„ìš”í•˜ë©´ 'ì •í™•í•œ ì •ì±…ì€ ì‚¬ë‚´ í¬í„¸ì—ì„œ í™•ì¸ í•„ìš”'ë¼ê³  ë§í•´ë¼."
    )
    user_prompt = f"ì§ˆë¬¸:\n{state['question']}\n\nì»¨í…ìŠ¤íŠ¸:\n{context}"
    out = llm.invoke([
        {"role": "system", "content": sys_prompt},
        {"role": "user", "content": user_prompt}
    ]).content
    
    return {**state, "reply": out, "sources": sources}

# ë„êµ¬(Tool) í•¨ìˆ˜ ê²°ê³¼ ì‚¬ìš©ì ì¹œí™”ì ì¸ í˜•íƒœë¡œ ë³€í™˜
def node_finalize(state: BotState) -> BotState:
    # ì´ ë…¸ë“œëŠ” ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ë¥¼ ì‚¬ìš©ì ì¹œí™”ì ì¸ ë©”ì‹œì§€ë¡œ ë³€í™˜
    res = state.get("tool_output", {})
    if state["intent"] == "direct_tool":
        if res.get("tool_name") == "tool_reset_password":
            text = f"âœ… ë¹„ë°€ë²ˆí˜¸ ì´ˆê¸°í™” ì•ˆë‚´\n\n" + "\n".join(f"{i+1}. {s}" for i,s in enumerate(res.get("steps", []))) if res.get("ok") else f"â—{res.get('message','ì‹¤íŒ¨')}"
        elif res.get("tool_name") == "tool_request_id":
            text = f"ğŸ†” ID ë°œê¸‰ ì‹ ì²­ ì ˆì°¨ ì•ˆë‚´\n\n" + "\n".join(f"{i+1}. {s}" for i,s in enumerate(res.get("steps", []))) if res.get("ok") else f"â—{res.get('message','ì‹¤íŒ¨')}"  
        elif res.get("tool_name") == "tool_owner_lookup":
            if "owners" in res:  # ì „ì²´ ì¡°íšŒ ì¼€ì´ìŠ¤
                lines = ["ğŸ“‹ ì „ì²´ ë‹´ë‹¹ì ëª©ë¡:"]
                for o in res["owners"]:
                    lines.append(
                        f"- í™”ë©´: {o.get('screen')} / ì´ë¦„: {o.get('owner')} / "
                        f"ì´ë©”ì¼: {o.get('email')} / ì—°ë½ì²˜: {o.get('phone')}"
                    )
                text = "\n".join(lines)
            else:  # ê¸°ì¡´ ë‹¨ì¼ ë‹´ë‹¹ì ì¡°íšŒ
                text = (
                    f"ğŸ‘¤ '{res.get('screen')}' ë‹´ë‹¹ì\n"
                    f"- ì´ë¦„: {res.get('owner', {}).get('owner')}\n"
                    f"- ì´ë©”ì¼: {res.get('owner', {}).get('email')}\n"
                    f"- ì—°ë½ì²˜: {res.get('owner', {}).get('phone')}"
                    if res.get("ok")
                    else f"â—{res.get('message','ì¡°íšŒ ì‹¤íŒ¨')}"
                )
        else: # FAQë„ ì—¬ê¸°ì„œ ì²˜ë¦¬
            text = res.get("answer", "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        # ğŸ’¡ ìˆ˜ì •: ë°˜í™˜ í‚¤ë¥¼ 'reply'ë¡œ í†µì¼
        return {**state, "reply": text}
    return state
# =============================================================
# 4. workflow/graph.py (LangGraph ê·¸ë˜í”„ ì •ì˜)
# ==========================================================
def load_owner_data() -> List[Dict[str, str]]:
    global _owner_data
    if _owner_data is not None:
        return _owner_data

    owner_file_path = constants.KB_DATA_DIR / "owners.csv"
    if not owner_file_path.exists():
        logger.warning(f"OWNER íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {owner_file_path}")
        _owner_data = []
        return _owner_data

    loaded_data = []
    try:
        # UTF-8 with BOMë„ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
        with open(owner_file_path, mode='r', encoding='utf-8-sig') as f:
            reader = csv.DictReader(f)
            for row in reader:
                if "screen" in row and "owner" in row:
                    # í˜•íƒœì†Œ ë¶„ì„ ë¶ˆí•„ìš” â†’ ê·¸ëŒ€ë¡œ ì €ì¥
                    loaded_data.append(row)
        logger.info(f"{len(loaded_data)}ê°œì˜ OWNER ë°ì´í„°ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        logger.error(f"OWNER íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        loaded_data = []
    _owner_data = loaded_data
    return _owner_data

# LangChain & LangGraph - Multi-Agent Flow ì„¤ê³„ ë° êµ¬í˜„ [checklist: 3,4,5]
# FAQ ë°ì´í„° ë¡œë“œ
def load_faq_data() -> List[Dict[str, str]]:
    global _faq_data
    # ì´ë¯¸ ë¡œë“œëœ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ë°”ë¡œ ë°˜í™˜
    if _faq_data is not None:
        return _faq_data
    
    faq_file_path = constants.KB_DEFAULT_DIR / "faq_data.csv"
    if not faq_file_path.exists():
        logger.warning(f"FAQ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {faq_file_path}")
        _faq_data = []
        return _faq_data
    
    loaded_data = []
    try:
        # íŒŒì¼ ë¡œë“œ ë° ë°ì´í„° íŒŒì‹±
        with open(faq_file_path, mode='r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            # Okt ê°ì²´ë¥¼ ë¯¸ë¦¬ ìƒì„±í•´ë‘ê³  ì¬ì‚¬ìš©
            okt_processor = get_okt()
            for row in reader:
                if "question" in row and "answer" in row:
                    # Okt ê°ì²´ë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì„ ë¶„ì„
                    row["faq_words"] = set(okt_processor.phrases(row.get("question", "")))
                    loaded_data.append(row)
        logger.info(f"{len(loaded_data)}ê°œì˜ FAQ ë°ì´í„°ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        logger.error(f"FAQ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        # ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ ë¹ˆ ëª©ë¡ì„ ë°˜í™˜
        loaded_data = []
    _faq_data = loaded_data
    return _faq_data

# FAQ ìœ ì‚¬ë„ ê²€ìƒ‰
def find_similar_faq(question: str) -> Optional[Dict[str, Any]]:
    logger.error(f"find_similar_faq - ì§ˆë¬¸ì€?: {question.lower().strip()}")
    faq_data = load_faq_data()
    if not faq_data: return None
    user_words = set(get_okt().phrases(question.lower()))
    if not user_words: user_words = set(get_okt().nouns(question.lower()))
    if not user_words: return None
    
    best_score = 0.0
    best_item = None
    
    for item in faq_data:
        faq_words = item.get("faq_words", set())
        if not faq_words: continue
        intersection = len(user_words.intersection(faq_words))
        union = len(user_words.union(faq_words))
        score = intersection / union if union > 0 else 0
        
        if score > best_score:
            best_score = score
            best_item = item

    # ì ìˆ˜ ì„ê³„ê°’(threshold)ì„ 0.2ë¡œ ì„¤ì •
    return best_item if best_score > 0.2 else None

# StateGraph í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•´ ë©€í‹° ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš°ë¥¼ ì •ì˜í•¨
# =============================================================
# 5. StateGraph í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•´ ë©€í‹° ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš°ë¥¼ ì •ì˜
# workflow/graph.py (LangGraph ê·¸ë˜í”„ ì •ì˜)
# =============================================================
_memory_checkpointer = MemorySaver()
def build_graph():
    logger.info("build_graph")
    g = StateGraph(BotState)
    g.add_node("classify", node_classify)
    g.add_node("greeting", node_greeting)
    g.add_node("direct_tool", node_direct_tool)
    g.add_node("faq", node_faq)
    g.add_node("rag", node_rag)
    g.add_node("finalize", node_finalize)
    
    # ì±—ë´‡ì˜ ì‹œì‘ì 
    g.set_entry_point("classify")

    # ì˜ë„ì— ë”°ë¼ ë…¸ë“œ ì—°ê²°
    g.add_conditional_edges(
        "classify",
        lambda s: s["intent"],
        {
            "greeting": "greeting",
            "direct_tool": "direct_tool",
            "faq": "faq",
            "agent_action": "rag",
            "general_qa": "rag", # ì¼ë°˜ ì§ˆë¬¸ì€ RAGë¡œ ë¼ìš°íŒ…
        }
    )

    # ë…¸ë“œ ì—°ê²°
    g.add_edge("greeting", END)
    g.add_edge("direct_tool", "finalize")
    g.add_edge("rag", END)
    
    # FAQ ë…¸ë“œì—ì„œ ë‹µë³€ì„ ì°¾ì§€ ëª»í•˜ë©´ RAGë¡œ ë‹¤ì‹œ ë¼ìš°íŒ…
    g.add_conditional_edges(
        "faq",
        lambda s: s["intent"],
        {
            "faq": "finalize", # FAQ ë‹µë³€ì„ ì°¾ì•˜ì„ ë•Œ
            "general_qa": "rag", # FAQ ë‹µë³€ì„ ëª» ì°¾ì•˜ì„ ë•Œ
        }
    )
    g.add_edge("finalize", END)
    
    return g.compile(checkpointer=_memory_checkpointer)
# =============================================================
# 6. Pipeline Orchestration
# workflow/pipeline.py (íŒŒì´í”„ë¼ì¸ ì‹¤í–‰)
# =============================================================
# LangChain & LangGraphë¥¼ í™œìš©í•œ ì „ì²´ íŒŒì´í”„ë¼ì¸ êµ¬ì„±
_graph = None
def run_graph_pipeline(question: str, session_id: str) -> Dict[str, Any]:
    # [checklist: 5] LangChain & LangGraph - ë©€í‹°í„´ ëŒ€í™” (memory) í™œìš©
    """LangGraph ê¸°ë°˜ì˜ AI íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
    global _graph
    # LangGraph Studio (UI ê¸°ë°˜ ê·¸ë˜í”„ ì‹œê°í™” íˆ´)ë¥¼ í™œìš©í•˜ì—¬ ê·¸ë˜í”„ ëª¨ë‹ˆí„°ë§ ë° ë””ë²„ê¹… - hold
    # LangSmith ì½œë°± í•¸ë“¤ëŸ¬ ì„¤ì •
    langsmith_api_key = os.getenv("LANGSMITH_API_KEY")
    langsmith_project_name = os.getenv("LANGSMITH_PROJECT", "Servicedesk Bot")
    
    callbacks = None
    if langsmith_api_key:
        # callbacks = CallbackManager([
        #     LangSmithCallbackHandler(
        #         project_name=langsmith_project_name, 
        #         api_key=langsmith_api_key, 
        #         session_id=session_id
        #     )
        # ])
        logger.info("LangSmith is enabled.")
    else:
        logger.info("LangSmith is not enabled. Provide LANGSMITH_API_KEY in .env to enable it.")

    logger.info("pipeline_in", extra={"extra_data": {"q": question}})
    if _graph is None: _graph = build_graph()
    
    out = _graph.invoke(
        input={"question": question, "intent":"", "reply":"", "sources":[], "tool_output":{}},
        config={"configurable": {"thread_id": session_id}, "callbacks": callbacks}
    )
    logger.info("pipeline_out", extra={"extra_data": {"intent": out.get("intent", "")}})
    
    # LangGraph ê²°ê³¼ì—ì„œ ìµœì¢… ë‹µë³€ê³¼ ì˜ë„ ë°˜í™˜
    return {
        "reply": out.get("reply", "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."),
        "intent": out.get("intent", "unsupported"),
        "sources": out.get("sources", []),
    }

def pipeline(question: str, session_id: str) -> Dict[str, Any]:
    """
    Azure ì—°ê²° ìƒíƒœì— ë”°ë¼ ì ì ˆí•œ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ìš”ì²­ì„ ë¼ìš°íŒ…í•©ë‹ˆë‹¤.
    """
    if AZURE_AVAILABLE:
        try:
            return run_graph_pipeline(question, session_id)
        except Exception as e:
            logger.error(f"ì£¼ìš” íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}. í´ë°± ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
            return {
                "reply": "ì£„ì†¡í•©ë‹ˆë‹¤. ì‹œìŠ¤í…œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.",
                "intent": "system_error",
                "sources": []
            }
    else:
        # í´ë°± ëª¨ë“œ
        logger.info("í’€ë°± íŒŒì´í”„ë¼ì¸")
        # 1. ì¸ì‚¬ë§ ì²˜ë¦¬
        logger.error(f"í’€ë°± íŒŒì´í”„ë¼ì¸ - ì§ˆë¬¸ì€?: {question.lower().strip()}")
        if question.lower().strip() in constants.GREETINGS:
            return {
                "reply": "ë„¤, ë°˜ê°‘ìŠµë‹ˆë‹¤. ë¬¸ì˜ì‚¬í•­ì„ ë§ì”€í•´ ì£¼ì‹œë©´ ì œê°€ ë„ì™€ë“œë¦´ê²Œìš”.",
                "intent": "greeting",
                "sources": []
            }     
        # 2. ë„êµ¬ ê´€ë ¨ í‚¤ì›Œë“œ ì²˜ë¦¬
        if "ë¹„ë°€ë²ˆí˜¸ ì´ˆê¸°í™”" in question:
            res = tool_reset_password.invoke({})
            return {
                "reply": f"âœ… ë¹„ë°€ë²ˆí˜¸ ì´ˆê¸°í™” ì•ˆë‚´\n\n" + "\n".join(f"{i+1}. {s}" for i, s in enumerate(res.get("steps", []))),
                "intent": "direct_tool",
                "sources": []
            }
        
        if "ì•„ì´ë”” ë°œê¸‰" in question or "ê³„ì • ë°œê¸‰" in question:
            res = tool_request_id.invoke({})
            reply_text = f"ğŸ†” ID ë°œê¸‰ ì‹ ì²­ ì ˆì°¨ ì•ˆë‚´\n\n" + "\n".join(f"{i+1}. {s}" for i, s in enumerate(res.get("steps", [])))
            
            return {
                "reply": reply_text,
                "intent": "direct_tool",
                "sources": []
            }
        if "ë‹´ë‹¹ì" in question:
            logger.info("ë‹´ë‹¹ì ì¡°íšŒ")
            owner_data = load_owner_data()

            # 1. ì „ì²´ ì¡°íšŒ ìš”ì²­ ì²˜ë¦¬
            if "ì „ì²´" in question or question.strip() == "ë‹´ë‹¹ì ì¡°íšŒ":
                if not owner_data:
                    return {
                        "reply": "ë“±ë¡ëœ ë‹´ë‹¹ì ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.",
                        "intent": "direct_tool",
                        "sources": []
                    }
                reply_lines = ["ğŸ“‹ ì „ì²´ ë‹´ë‹¹ì ëª©ë¡"]
                for item in owner_data:
                    reply_lines.append(
                        f"- {item.get('screen')}: {item.get('owner')} "
                        f"(ğŸ“§ {item.get('email')}, â˜ {item.get('phone')})"
                    )
                return {
                    "reply": "\n".join(reply_lines),
                    "intent": "direct_tool",
                    "sources": [{"source": "owner.csv"}]
                }

            # 2. ê°œë³„ screen ì¡°íšŒ ì²˜ë¦¬ (ì •í™•íˆ ì¼ì¹˜í•  ë•Œ)
            for item in owner_data:
                
                #if question.strip() == item.get("screen", "").strip():
                if item.get("screen", "").strip() and item.get("screen", "").strip() in question:
                    return {
                        "reply": (
                            f"ğŸ‘¤ '{item.get('screen')}' ë‹´ë‹¹ì\n"
                            f"- ì´ë¦„: {item.get('owner')}\n"
                            f"- ì´ë©”ì¼: {item.get('email')}\n"
                            f"- ì—°ë½ì²˜: {item.get('phone')}"
                        ),
                        "intent": "direct_tool",
                        "sources": [{"source": "owner.csv"}]
                    }

            # 3. ëª» ì°¾ì•˜ì„ ë•Œ
            return {
                "reply": "ë‹´ë‹¹ìë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. í™”ë©´/ë©”ë‰´ëª…ì„ ì •í™•íˆ ì…ë ¥í•´ ì£¼ì„¸ìš”.",
                "intent": "direct_tool",
                "sources": []
            }

                # if "ë‹´ë‹¹ì" in question:
        #     screen = "ì¸ì‚¬ì‹œìŠ¤í…œ-ì‚¬ìš©ìê´€ë¦¬" if "ì¸ì‚¬ì‹œìŠ¤í…œ" in question else None # screenì´ ì—†ì„ ê²½ìš° Noneìœ¼ë¡œ ì„¤ì •
        #     if screen:
        #         payload = {"screen": screen}
        #         res = tool_owner_lookup.invoke({"payload": {"screen": screen}})
        #         # Pydantic ì˜¤ë¥˜ ìˆ˜ì •: payload ì¸ìë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë˜í•‘
        #         #res = tool_owner_lookup.invoke({"payload": {"screen": screen}})
        #         #res = tool_owner_lookup.invoke(payload)
        #         #res = tool_owner_lookup.invoke(**payload) # **ë¡œ ì–¸íŒ©í•˜ì—¬ í˜¸ì¶œ
        #         #res = tool_owner_lookup.invoke(input=payload) # ìˆ˜ì •ëœ í˜¸ì¶œ ë°©ì‹
        #     else:
        #         # screenì´ ì—†ëŠ” ê²½ìš°, ë¹ˆ ë”•ì…”ë„ˆë¦¬ë¥¼ inputìœ¼ë¡œ ì „ë‹¬
        #         #res = tool_owner_lookup.invoke({})
        #         res = tool_owner_lookup.invoke(input={})
        #         # ë‹´ë‹¹ì ëª©ë¡ ì „ì²´ë¥¼ ì•ˆë‚´í•˜ëŠ” ë©”ì‹œì§€ë¥¼ ë°˜í™˜
        #         reply = "ë‹´ë‹¹ìë¥¼ ì°¾ê¸° ìœ„í•œ í™”ë©´ì´ë‚˜ ë©”ë‰´ ì •ë³´ê°€ í•„ìš”í•©ë‹ˆë‹¤. ì˜ˆ: 'ì¬ë¬´ì‹œìŠ¤í…œ ë‹´ë‹¹ì ì•Œë ¤ì¤˜'"
        #         return {
        #             "reply": reply,
        #             "intent": "direct_tool",
        #             "sources": []
        #         }

        # 3. FAQ ê²€ìƒ‰ì„ ë„êµ¬ í‚¤ì›Œë“œë³´ë‹¤ ë¨¼ì € ìˆ˜í–‰í•˜ì—¬ RAG í…ŒìŠ¤íŠ¸ í†µê³¼
        faq_item = find_similar_faq(question)
        if faq_item:
            return {
                "reply": f"[ì•ˆë‚´] ë¬¸ì˜í•˜ì‹  ë‚´ìš©ì— ëŒ€í•œ ë‹µë³€ì…ë‹ˆë‹¤.\n\n---\n\n{faq_item.get('answer')}",
                "intent": "faq",
                "sources": [{"source": "faq_data.csv"}]
            }

        # 4. ê·¸ ì™¸ ëª¨ë“  ì§ˆë¬¸ì— ëŒ€í•œ í´ë°±
        return {
            "reply": "ì£„ì†¡í•©ë‹ˆë‹¤. ë¬¸ì˜í•˜ì‹  ë‚´ìš©ì„ ì´í•´í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.",
            "intent": "unsupported",
            "sources": []
        }

